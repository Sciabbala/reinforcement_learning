{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning with a DOOM environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was created thanks to a course on Deep Reinforcement Learning created by Thomas Simonini. You can find the syllabus here: https://simoninithomas.github.io/Deep_reinforcement_learning_Course/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Q-Learning** is a way to apply the concepts of Q-Learning to environments that have millions of different states and therefore are unsuitable to be tackled by an agent using a Q-Table. Therefore we will train a neural network that will try to **approximate the Q-values** for each possible action in the given environment.\n",
    "\n",
    "The objective of this notebook is to train an agent through a **Neural network** to play a DOOM environment with the following rules:\n",
    "\n",
    "- A monster is spawned randomly somewhere along the opposite wall.\n",
    "- Player can only go left/right and shoot.\n",
    "- 1 hit is enough to kill the monster.\n",
    "\n",
    "- Episode finishes when monster is killed or on timeout (300).\n",
    "\n",
    "The **rewards** are as follows:\n",
    "- +101 for killing the monster\n",
    "- -5 for missing\n",
    "- Episode finishes and monster is still alive = -1\n",
    "\n",
    "\n",
    "Since we're dealing with image frames from a video game we will use a **Convolutional Neural Network (CNN)** given their performance and efficiency in dealing with images. This an example frame from the DOOM environment that we will be using:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"doom_frame.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decrease the processing load and duration of the training I will first pre-process the image. First of all, the image can be converted to **grayscale** since there's no need for color to recognize the monster from the rest of the frame. This already saves a lot of time given that we're dealing with a **single channel** and not 3 (RGB).\n",
    "\n",
    "Then the roof can be cut out of the frame since the monster cannot appear on that part of the frame. There is also no need to use the full resolution of the environment therefore the image can be **downscaled to a lower resolution**.\n",
    "\n",
    "An important point to make is that we will feed the neural the network with 4 stacked frames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
