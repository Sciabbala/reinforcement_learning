{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning with OpenAI Taxi v3\n",
    "\n",
    "This notebook was created thanks to a course on Deep Reinforcement Learning created by Thomas Simonini. You can find the syllabus here: https://simoninithomas.github.io/Deep_reinforcement_learning_Course/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$G_{t}=\\sum_{k=0}^{\\infty} \\gamma^{k} R_{t+k+1} w h e r e  \\gamma \\in[0,1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula above represents the **discounted cumulative expected rewards**. It's the sum of the rewards given by the environment at each time step $t$ discounted by $\\gamma$. This is a **discount rate** (between 0 and 1) used to increase the effect of **long-term rewards** in earlier time steps and then, as the end of the game approaches and future rewards are less likely to happen, gets smaller and smaller to prioritize **short-term** rewards. This happens because $\\gamma$ is elevated to the current time step $k$ and therefore it will be become smaller and smaller with higher $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important concept is the **exploration/exploitation** tradeoff. At the start of the training the Q-table is going to be completely useless since it is going to be full of arbitrary values (usually 0s) that weren't the result of the agent playing. Before it actually makes sense to follow the Q-table when deciding on action is is better to have the agent focus on **exploration**, that is making random actions for the sake of exploring the state of game and start filling up the Q-table according to the rewards received.\n",
    "\n",
    "In the code below this is represented by the **epsilon** variable. This variable gets smaller each **episode** (that is, each game played by the agent) so that there will be a strong focus on random moves at the **start** of the training so that the Q-table can be populated and then, as the training progresses, **epsilon** will get smaller and more and more actions of the agent will be decided based on highest Q-table value for that specific game state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will implement an agent that can play the OpenAI Gym environment Taxi-v3. The goal of the game is to pick up the passenger at one location and drop him off in another. There are 4 different location marked by 4 different letters. The point system for this environment works as follows:\n",
    "\n",
    "- You receive +20 points for a successful dropoff\n",
    "- Lose 1 point for every timestep it takes.\n",
    "- There is also a 10 point penalty for illegal pick-up and drop-off actions (if you don't drop the passenger in one of the 3 other locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The libraries needed are:\n",
    "- Numpy to generate the Qtable\n",
    "- Gym for the Taxi environment\n",
    "- Random to generate random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | :\u001b[43m \u001b[0m|\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating Taxi environment and rendering an example game state\n",
    "env = gym.make(\"Taxi-v3\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action size  6\n",
      "State size  500\n"
     ]
    }
   ],
   "source": [
    "# Obtaining number of possible actions (number of rows of Q-table) and\n",
    "# number of possible states (number of columns)\n",
    "action_size = env.action_space.n\n",
    "print(\"Action size \", action_size)\n",
    "\n",
    "state_size = env.observation_space.n\n",
    "print(\"State size \", state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Creating empty Q-table\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting hyperparameters\n",
    "total_episodes = 50000 # Total train episodes\n",
    "total_test_episodes = 1000 \n",
    "max_steps = 99\n",
    "\n",
    "learning_rate = 0.7\n",
    "gamma = 0.618\n",
    "\n",
    "# setting exploration parameters\n",
    "epsilon = 1.0\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(total_episodes):\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    \n",
    "    # Executing up to 99 actions\n",
    "    for step in range(max_steps):\n",
    "        # Obtaining random integer.\n",
    "        exp_exp_tradeoff = random.uniform(0,1)\n",
    "        # If tradeoff > epsilon the action will be based on the biggest Q-value for this state\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = np.argmax(qtable[state,:])\n",
    "        # else the action will be random (exploring the environment)\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        # Passing the action into the step method of the environment\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Updating the Q-table with new value based on the results of the last action\n",
    "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma *\n",
    "                                    np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "        \n",
    "        # Overwriting old state to use new_state of the environment in next loop\n",
    "        state = new_state\n",
    "        \n",
    "        # Break the loop if the agent finishes the game\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    # Updating epsilon to decrease the ratio of exploration/exploitation over time\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score over time: 7.922\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "rewards = []\n",
    "\n",
    "for episode in range(total_test_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(qtable[state,:])\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        if done:\n",
    "            rewards.append(total_rewards)\n",
    "            break\n",
    "        state = new_state\n",
    "env.close()\n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_test_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the need to try out different parameters I have created two functions in the `qlearning_functions.py` file to test various hyperparameters and compare the scores. This will mostly come down to playing out with different values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qlearning_functions import agent_testing, agent_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing learning_rate values of 0.1, 0.4, 0.7 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_qtables = [agent_training(env, qtable, learning_rate=learning_rate) \n",
    "                   for learning_rate in [0.1, 0.4, 0.7, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [agent_testing(env, qtable=trained_table) for trained_table in trained_qtables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7.905', '7.906', '7.876', '7.865']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing gamma values of 0.4, 0.5, 0.6 and 0.7 together with the learning rate associated with the highest score, 0.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_qtables = [agent_training(env, qtable, learning_rate=0.4, gamma=gamma) \n",
    "                   for gamma in [0.4, 0.5, 0.6, 0.7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [agent_testing(env, qtable=trained_table) for trained_table in trained_qtables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7.504', '7.945', '8.023', '2.424']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing higher epsilon decay rates of 0.03, 0.07, 0.1 and 0.15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_qtables = [agent_training(env, qtable, learning_rate=0.4, gamma=0.6, decay_rate=decay_rate) \n",
    "                   for decay_rate in [0.03, 0.07, 0.1, 0.15]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [agent_testing(env, qtable=trained_table) for trained_table in trained_qtables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7.823', '7.868', '7.967', '7.957']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
